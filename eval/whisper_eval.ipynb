{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedlium_path = '/mnt/data/es-es/data'\n",
    "mls_path = '/mnt/data/mls_spanish_opus'\n",
    "model_id = \"openai/whisper-tiny\" # opciones: openai/whisper-medium openai/whisper-small openai/whisper-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "device = \"cuda:0\"\n",
    "torch_dtype = torch.float32\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from yaml import safe_load\n",
    "\n",
    "def extract_audio_metadata(path):\n",
    "    info = sf.info(path)\n",
    "    return {'filename':path, 'samplerate': info.samplerate, 'duration': info.duration}\n",
    "\n",
    "def read_mls(path):\n",
    "    all_dfs = []\n",
    "    for split in ['test']:\n",
    "        df = pd.read_csv(Path(path,split,'transcripts.txt'),delimiter='\\t',header=None,names=['idx','transcription'])\n",
    "        all_wavs = Path(path,split,'audio').rglob('*.opus')\n",
    "        wav_mapping = {x.stem: str(x.resolve()) for x in all_wavs}\n",
    "        df['filename'] = df['idx'].apply(lambda x: wav_mapping[x])\n",
    "        df['partition'] = split\n",
    "        df['start'] = 0\n",
    "        all_dfs.append(df)\n",
    "    df = pd.concat(all_dfs)\n",
    "    metadatas = []\n",
    "    for f in tqdm(df['filename']):\n",
    "        metadatas.append(extract_audio_metadata(f))\n",
    "    metadatas = pd.DataFrame(metadatas)\n",
    "    df = pd.merge(df, metadatas, left_on='filename', right_on='filename')\n",
    "    df['dataset'] = 'mls'\n",
    "    return df\n",
    "\n",
    "def read_tedlium(path):\n",
    "    all_dfs = []\n",
    "    for split in ['test']:\n",
    "        txt_path, wav_path = Path(path, split, 'txt'), Path(path, split, 'wav')\n",
    "        transcripts = load_tedlium_transcripts(txt_path / f'{split}.es')\n",
    "        with (txt_path / f'{split}.yaml').open('r') as f:\n",
    "            audio_metadata = safe_load(f)\n",
    "        for i in tqdm(range(len(audio_metadata))):\n",
    "            audio = audio_metadata[i]\n",
    "            audio['transcription'] = transcripts[i]\n",
    "            audio['wav'] = audio['wav'].replace('wav', 'flac')\n",
    "            audio['partition'] = split if split != 'valid' else 'dev'\n",
    "            audio['filename'] = str((wav_path / audio['wav']).resolve())\n",
    "            audio['samplerate'] = extract_audio_metadata(audio['filename'])['samplerate']\n",
    "            audio['start'] = audio['offset']\n",
    "            del audio['wav']\n",
    "            del audio['offset']\n",
    "            del audio['speaker_id']\n",
    "        split_df = pd.DataFrame(audio_metadata).reset_index(names='idx')\n",
    "        all_dfs.append(split_df)\n",
    "    df = pd.concat(all_dfs)\n",
    "    df['dataset'] = 'tedlium'\n",
    "    return df\n",
    "        \n",
    "def load_tedlium_transcripts(file_path):\n",
    "    transcripts = []\n",
    "    with file_path.open('r') as f:\n",
    "        for line in f:\n",
    "            line = line.lower()[:-2]\n",
    "            transcripts.append(line)\n",
    "    return transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2385/2385 [00:00<00:00, 16616.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1012/1012 [00:00<00:00, 23664.27it/s]\n"
     ]
    }
   ],
   "source": [
    "df_mls = read_mls(mls_path)\n",
    "df_tedlium = read_tedlium(tedlium_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from whisper_normalizer.basic import BasicTextNormalizer\n",
    "from jiwer import wer, cer\n",
    "from pandas import DataFrame\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "\n",
    "def evaluate_df(df):\n",
    "    results = {'gt': [], 'pred': [], 'wer': [], 'cer': [], 'time': []}\n",
    "    text_normalizer = Normalizer(input_case='cased', lang='es')\n",
    "    second_normalizer = BasicTextNormalizer()\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        start = row['start']\n",
    "        duration = row['duration']\n",
    "        filename = row['filename']\n",
    "        og_transcription = second_normalizer(text_normalizer.normalize(row['transcription']))\n",
    "        wav, _ = librosa.core.load(filename, offset=start, duration=duration)\n",
    "        start_time = time.time()\n",
    "        transcription = pipe(wav, generate_kwargs={\"language\": \"spanish\"})\n",
    "        results['time'].append(round(time.time() - start_time, 10))\n",
    "        transcription = second_normalizer(text_normalizer.normalize(transcription['text']))\n",
    "        results['gt'].append(og_transcription)\n",
    "        results['pred'].append(transcription)\n",
    "        results['wer'].append(wer(og_transcription, transcription))\n",
    "        results['cer'].append(cer(og_transcription, transcription))\n",
    "    return DataFrame(results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars. This might take some time...\n",
      "49it [00:10,  4.84it/s]/mnt/ssd1/lpepino/miniconda3/envs/llm-asr/lib/python3.9/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "57it [00:13,  4.09it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "120it [00:29,  2.78it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "207it [00:55,  2.67it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "215it [00:57,  5.49it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "337it [01:29,  3.80it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "339it [01:29,  2.96it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "415it [01:49,  4.92it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "442it [01:55,  8.98it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "628it [02:42, 10.24it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "700it [02:50,  6.52it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "832it [03:22,  2.71it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "841it [03:25,  4.38it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "849it [03:27,  5.24it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "900it [03:39,  5.07it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "925it [03:45,  4.52it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "946it [03:50,  2.71it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "997it [04:03,  4.53it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "1012it [04:06,  4.11it/s]\n",
      " NeMo-text-processing :: INFO     :: Creating ClassifyFst grammars. This might take some time...\n",
      "108it [00:54,  1.45it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "148it [01:15,  2.10it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "172it [01:26,  2.79it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "187it [01:32,  2.19it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "188it [01:33,  2.08it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "195it [01:36,  2.24it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "211it [01:43,  1.92it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "337it [02:47,  1.21it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "356it [03:04,  1.12it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "362it [03:09,  1.25it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "363it [03:10,  1.08it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "367it [03:14,  1.09it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "463it [04:16,  1.74it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "692it [06:33,  1.42it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "1090it [10:25,  1.98it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "1253it [11:55,  1.24it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "1545it [14:54,  1.39it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "1559it [15:06,  1.11it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "1602it [15:41,  1.20it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "1737it [17:01,  1.85it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "1938it [18:51,  2.35it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "2385it [23:02,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = model_id.split('/')[-1]\n",
    "df_tedlium_results = evaluate_df(df_tedlium)\n",
    "df_tedlium_results.to_csv(f'tedlium_results_{model_name}.csv', index=False)\n",
    "df_mls_results = evaluate_df(df_mls)\n",
    "df_mls_results.to_csv(f'mls_results_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-ASR",
   "language": "python",
   "name": "llm-asr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
