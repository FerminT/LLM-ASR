{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad1d6d8-0e75-47b6-9ea9-b7332f394300",
   "metadata": {},
   "source": [
    "Speech encoder -> [BS, T, D]\n",
    "Transcripcion -> Tokenizer -> [BS, lentrans] -> Embedding -> [BS, lentrans, D]\n",
    "\n",
    "concat(axis=-1) -> [BS, T+lentrans, D]\n",
    "loss(T:)\n",
    "\n",
    "Encoder -> Decoder\n",
    "Audio Texto\n",
    "\n",
    "Solo Decoder -> Audio + Texto\n",
    "\n",
    "x[:-1] -> LLM -> x[1:]\n",
    "\n",
    "Agregar a la transcripcion un <eos> al final\n",
    "\n",
    "Dataset -> {'wav': numpy, 'transcription': numpy (ids)}\n",
    "Dataloader -> [{}, {}] -> collate_fn -> {'wav': tensor paddeado, 'wav_lens': tensor lens, 'transcription': tensor paddeado, 'transcription_lens': tensor lens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fcf8b36-1d2f-401f-be4a-7ccf853e7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, WavLMModel\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8e1205-3cf6-453c-967b-f372499cdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFLLMModel(torch.nn.Module):\n",
    "    def __init__(self, hf_path):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(hf_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
    "\n",
    "    def forward(self, x, attention_mask, **kwargs):\n",
    "        return self.model(inputs_embeds=x, attention_mask=attention_mask, **kwargs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_lut(self):\n",
    "        pass\n",
    "\n",
    "class WavLM(torch.nn.Module):\n",
    "    def __init__(self, hf_path, layer=12):\n",
    "        super().__init__()\n",
    "        self.model = WavLMModel.from_pretrained(hf_path)\n",
    "        self.downsampling = 320\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.stack(self.model(x,output_hidden_states=True)['hidden_states'])[self.layer]\n",
    "\n",
    "class GPTModel(HFLLMModel):\n",
    "    def get_lut(self):\n",
    "        return self.model.transformer.wte\n",
    "\n",
    "class LLMASR(pl.LightningModule):\n",
    "    def __init__(self, llm_model, wav_model):\n",
    "        super().__init__()\n",
    "        self.llm_model = llm_model\n",
    "        self.llm_model_lut = self.llm_model.get_lut()\n",
    "        self.wav_model = wav_model\n",
    "\n",
    "    def prepare_input(self, speech, transcription, speech_lens, transcription_lens):\n",
    "        x = []\n",
    "        speech = self.wav_model(speech)\n",
    "        speech_lens = speech_lens//self.wav_model.downsampling\n",
    "        transcription = self.llm_model_lut(transcription)\n",
    "        for s,sl,t,tl in zip(speech, speech_lens, transcription, transcription_lens):\n",
    "            si = s[:sl]\n",
    "            ti = t[:tl]\n",
    "            xi = torch.cat([si,ti],axis=0)\n",
    "            x.append(xi)\n",
    "        xlens = [len(xi) for xi in x]\n",
    "        maxlen = max(xlens)\n",
    "        x = [torch.nn.functional.pad(xi,(0,0,0,maxlen - xi.shape[0])) for xi in x]\n",
    "        xlens = torch.tensor(xlens)\n",
    "        speech_lens = torch.tensor(speech_lens)\n",
    "        padding_mask = torch.arange(0,maxlen)[None,:] < xlens[:,None]\n",
    "        response_mask = torch.logical_and(torch.arange(0,maxlen)[None,:] >= speech_lens[:,None],torch.arange(0,maxlen)[None,:] < xlens[:,None])\n",
    "        return torch.stack(x), padding_mask, response_mask\n",
    "        \n",
    "    def forward(self, speech, transcription, speech_lens, transcription_lens):\n",
    "        xin, padding_mask, response_mask = self.prepare_input(speech, transcription, speech_lens, transcription_lens)\n",
    "        return self.llm_model(xin[:,:-1], attention_mask = padding_mask), response_mask[:,1:], xin[:,1:]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        response_mask, logits, ytrue = self(batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa3413c4-7def-44a1-84ef-44dea8fe3bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "llm_model = GPTModel('DeepESP/gpt2-spanish')\n",
    "wav_model = WavLM('microsoft/wavlm-base-plus')\n",
    "asr_model = LLMASR(llm_model, wav_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31436730-1ba8-4290-ae87-fd2dc6ad8cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1468, 334, 21, 420, 50127, 40, 23], [1468, 334, 50256, 50256, 50256, 50256, 50256]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.tokenizer(['hola, como andas?.','hola'], add_special_tokens=True, padding=True, truncation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "440686e6-9da9-4eff-b71b-f302a2ab8427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4af21880-0cd2-493f-8220-f196fb2ec77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = torch.randn((4,32000))\n",
    "transcription = torch.randint(low=0,high=10000,size=(4,30))\n",
    "speech_lens = torch.tensor([32000,16000,24000,12000])\n",
    "transcription_lens = torch.tensor([30,10,15,20])\n",
    "\n",
    "out = asr_model(speech, transcription,  speech_lens, transcription_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e978b46f-7c9f-40bc-8447-2fcb0b859505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
